\chapter{Replication Strategies to Increase Storage Robustness in Decentralized P2P Architectures}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
\begin{center}
Brendan ~Benshoof
\end{center}


% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area


% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Classic P2P key-value storage systems use a small section of strategies to ensure that records are maintained in the system despite constant churn.
These strategies see effective levels of robustness to churn, however they often do not provide effective robustness against systemic failures due to natural disasters and network partitioning in the Internet or the overlay topology of the P2P systems.
We explore and evaluate the status quo of replica robustness strategies in the face of partition failures and propose new techniques to improve over the established methods.
\end{abstract}



\section{Introduction}

In this paper, we will consider a selection of strategies to increase robustness of storage in a p2p network.
Each strategy will be described, analyzed and finally compared and contrasted with other strategies.


Current DHTs are oprimized to store a large number of smaller records.
While is it is perfectly viable to link together such records to store larger files (and even use it as a shared file system), it has generally been used as a layer of indirection and provides means of discovering high bandwidth protocols. For example, Bitorent uses a DHT to help find peers for particular torrent rather than storing the files directly on the DHT.
If we maintain this trend of only storing small records on the DHT, it stands out as a clear choice for use a shared DNS cache, or as a mechanism for storing encrypted cryptographic keys.
All of these  use cases are predicated on a DHT being a reliable store of data.

\section{Robustness}

\subsection{Robustness and Churn}
Most DHTs focus their efforts on preventing record loss due to churn.
Records are lost to churn under two conditions: the node hosting the replica leaves the network or the node with a replica ceases to be responsible for the record due to a new node joining the network and claiming that portion of the address space. 
We will describe churn as a ``Replacement ratio'': $R_{c}$, which is measured over a period of time (most often a day).
This value describes the portion of DHT's metric space that is maintained by a different node at the end of the period.
This value is related to the more conventional churn rate metric $\frac{exits + joins}{2*size}$ but provides more information on the viability of records.

\subsection{Robustness and network partitions}
Network partitions are when a failure results in the network separating into multiple non-connected networks.
This can be a result of failures in the underlay and the overlay networks.
The result of this, is that unlike the churn based failures, failure of nodes is not independent.
For example, if two previously connected regions of the Internet cease to be connected due to disaster or political intervention, there will be two new networks, each having just lost all nodes in the other partition simultaneously.
This means that robustness methods based periodically re-storing records after they are lost are likely to be insufficient as all the possessors of a record may be found only in one of the partitions.

In practice, overlay partitions may occur due to eclipse attacks and logic errors in how the DHT constructs the overlay.
We will discuss overlay partitions in terms of a ratio $R_{o}$ that describes the size of a connected subnetwork that remains after a partition.
It is worth considering, that in the case of any partition, the ideal behavior is that both resulting partitions remain connected, search-able, and retain findable replicas of all records.


\subsubsection{Underlay Partitions}
An underlay partition can happen as result in a failure of infrastructure, manipulation of BGP, or governmental action.
Assuming the overlay network of the DHT has been constructed randomly in relation to the underlay network, the failures due to the underlay partition will essentially occur at random locations in the network. 

While this may resemble how failure occurs during churn, the failures will all occur effectively simultaneously, to a potentially very large segment of the population of the network.
Kademlia's topology is likely to recover from such an event, however searching the network will be impaired while new connections are established.
Chord's consistent topology proof is built upon an invariant that any join or exit from the network is occurring when the majority of the network is consistent. 
The sudden failure of nodes in a Underlay partition situation violates this assumption and may destroy the ability of the remaining chord networks to form a searchable overlay topology.

We will discuss underlay partitions in terms of a ratio $R_{u}$ that describes the fraction of randomly distributed members of the network that remain after an underlay partition occurs. 

\section{Passive Replication Strategies}

When DHTs are formalized\cite{chord}\cite{kademlia}, replication strategies are not discussed.
Passive strategies are those where a client writes the record and replicas to the DHT once, then no participant ever re-publishes the record.
Because of constant churn, such records are likely doomed to be lost as the nodes to which they were stored leave the network due to churn.
Because partition failures are being analyzed as specific events, rather than behavior over time, passive strategies will often fair similarly to active strategies because there will not have been sufficient time for nodes or sponsors to take action in response to the partition (If they realize a partition has occurred at all).

\subsection{k-random nodes}
The K-random node strategy is not used by any established DHT, however it provides a simple analytical model, which we can extend to the other replica strategies.
In the K-random node strategy, a file is stored at K locations chosen by chaining of a cryptographic hash.
That is, if a record is stored a location $L$, the first replica will be stored at location $hash(L)$ and the second at $hash(hash(L))$ and similarly re-hashing the identifier from the previous step until K nodes have been stored.

This scheme allows for simple speedup and redundancy.
Given a location $L$, any node can locate the potential k backup sites and search them by order of closeness or in parallel (effectively in order of latency).
Because the locations are effectively random, we can simplify our analysis for churn and partition tolerance.

The half-life due to churn for the record (a time period over which, on average, the number of replicas in the network will be halved ) is $\frac{-\log(2)}{\log{1-R_{c}}}$ and because records are a small integer, we can also consider an expected mean lifetime of a record in the network: $\frac{\log{K}}{\log(2)} * \frac{-\log(2)}{\log{1-R_{c}}}$. This value describes the time period after which the expected mean number of remaining replicas is 1. Because this 1 is the mean value, there is a 50\% likelihood there are less than 1 replica (or zero replicas, as the number of replicas is a natural number that cannot take partial values.)

This means we can present the average lifetime of a record in the network in terms of the number of replicas and churn replacement rate to be $O(\frac{-\log{K}}{\log{1-R_{c}}})$.
This implies that while more replicas always results in a longer expected lifetime, the returns on adding additional replicas diminishes quickly.

The random-k strategy preforms similarly in bother underlay and overlay partition failures (as the replicas in the network are effectively random in relation to each type of failure).
The expected fraction of surviving nodes is simply the $R$ fraction of the original network that the partition represents and the likelihood that a record is totally lost is simply the odds that all replicas are not in the considered partition: $(1-R)^{K}+R^{K}$. 


\subsection{k-nearest nodes}
K-nearest replication is a common strategy in Kademlia based networks.
When storing a record members preform a multi-beam search to discover the k closest to the target nodes.
This has an advantage over K-random nodes in that in many cases of failure there is no downtime.
If the current owner of a record dies, an adjacent node that likely already has a backup takes over responsibility.
In terms of churn resistance and underlay failure, K-nearest behaves identically to k-random simply because the likelihood of loss due to churn is independent of the replica's location in the network.

In the case of an overlay partition, the k-nearest strategy is less robust because it is more likely that all the nodes in the network fall on the larger side of the partition.


  


\section{Active Replication Strategies}


\subsection{k-Replica}
\subsection{Adaptive Replica (IRM)}

\section{Active Replication Strategies}
\subsection{Active sponsorship}
Active sponsorship is part of the method used to greatest effect in practice (it is often combined with a passive replication strategy).
When a member adds records to the network, it periodically re-adds the records such that the records are ensured to exist in a reachable location.
This method is specifically designed to combat churn related data loss.
It's success is predicated on the idea that the member adding the file will be more reliable then most nodes in the network and will persist over time.
Often the sponsorship is shared between many members who have a vested interest in the existence of the record.
Uptime is a function of the rate of sponsorship, number of sponsors, churn rate and any passive strategies used to augment the system.
\subsection{p2p backups}
\subsection{Replication within subnetworks}

\subsection{Active Sponsorship}

\subsection{Redundant Subnetworks}

\section{analysis}

\section{Conclusions}

